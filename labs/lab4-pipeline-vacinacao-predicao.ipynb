{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Recomendação de Vacinas\n",
    "\n",
    "Este notebook executa o pipeline de recomendação de vacinas para cães e gatos. As células abaixo irão compilar o código MapReduce, ingerir dados do PostgreSQL com Sqoop, executar o job no Hadoop, verifica os resultados e armazena os dados resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define qual implementação de MapReduce usar: 'java' ou 'python'\n",
    "MAPREDUCE_LANG = 'java'  # Altere para 'python' para usar a implementação em Python\n",
    "os.environ['MAPREDUCE_LANG'] = MAPREDUCE_LANG\n",
    "\n",
    "print(f\"Usando implementação de MapReduce: {os.environ['MAPREDUCE_LANG'].upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilando a Aplicação MapReduce\n",
    "\n",
    "A primeira etapa é compilar nosso código-fonte Java em um arquivo `.jar` executável. A célula abaixo usa o Maven para isso. Ela navega até o diretório do projeto e executa o `mvn package`. A flag `-q` é para uma saída mais limpa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$MAPREDUCE_LANG\" == \"java\" ]; then\n",
    "    cd vaccine-recommendation\n",
    "    mvn package -q\n",
    "    echo \"JAR compilado com sucesso em: target/vaccine-recommendation-1.0-SNAPSHOT.jar\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultando caderneta e o ciclo vacinal\n",
    "\n",
    "Execute a célula abaixo para se conectar novamente e fazer uma consulta `SELECT` com os dados inseridos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# As credenciais e o host são baseados no arquivo docker-compose.txt\n",
    "DB_HOST = \"localhost\" # Nome do serviço no Docker Compose\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_USER_PWD = \"postgres\"\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\"\"\"SELECT\n",
    "        p.pet_id,\n",
    "        p.species,\n",
    "        p.birth_date,\n",
    "        vr.vaccine_reference_id,\n",
    "        vr.vaccine_name,\n",
    "        vr.description,\n",
    "        vr.target_species,\n",
    "        vr.first_dose_age_months,\n",
    "        vr.booster_interval_months,\n",
    "        vr.mandatory,\n",
    "        vc.application_date\n",
    "    FROM pet p\n",
    "    CROSS JOIN vaccine_reference vr\n",
    "    LEFT JOIN vaccination_record vc ON p.pet_id = vc.pet_id AND vr.vaccine_reference_id = vc.vaccine_reference_id\n",
    "    WHERE p.ignore_recommendation = false AND p.nenabled = TRUE AND vr.nenabled = TRUE\"\"\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    print(\"Recomendações vacinais encontradas:\")\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")\n",
    "finally:\n",
    "    if 'conn' in locals() and conn is not None:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar recomendação vacinal com base na Caderneta de vacinação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Database connection details\n",
    "DB_HOST = \"localhost\"\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_USER_PWD = \"postgres\"\n",
    "\n",
    "pipeline_name = 'vaccine_recommendation'\n",
    "start_time = datetime.now()\n",
    "status = 'RUNNING'\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Insert a new record into the execution_history table\n",
    "    cur.execute(\n",
    "        \"INSERT INTO execution_history (target_table, start_time, status) VALUES (%s, %s, %s) RETURNING execution_id\",\n",
    "        (pipeline_name, start_time, status)\n",
    "    )\n",
    "    execution_id = cur.fetchone()[0]\n",
    "    conn.commit()\n",
    "\n",
    "    # Store the execution_id for later use\n",
    "    os.environ['EXECUTION_ID'] = str(execution_id)\n",
    "\n",
    "    print(f\"Execution started for pipeline: {pipeline_name}\")\n",
    "    print(f\"Execution ID: {execution_id}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if 'conn' in locals() and conn is not None:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ingestão da caderneta e o ciclo vacinal com Sqoop\n",
    "\n",
    "Importar os dados da cadernata de vacinação e do ciclo vacinal do PostgreSQL para o HDFS. O comando `hdfs dfs -rm` é usado para remover o diretório de destino antes da importação, garantindo que possamos executar esta célula várias vezes sem erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "INPUT_DIR=/petshop/input_vaccine_recommendation\n",
    "\n",
    "# Remove o diretório de entrada se ele já existir\n",
    "hdfs dfs -test -d $INPUT_DIR\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Removendo diretório HDFS existente: $INPUT_DIR\"\n",
    "    hdfs dfs -rm -r $INPUT_DIR\n",
    "fi\n",
    "\n",
    "echo \"Iniciando importação com Sqoop...\"\n",
    "sqoop import \\\n",
    "    --connect jdbc:postgresql://localhost:5432/postgres \\\n",
    "    --username postgres \\\n",
    "    --password postgres \\\n",
    "    --query \"SELECT \\\n",
    "        p.pet_id, \\\n",
    "        p.species, \\\n",
    "        p.birth_date, \\\n",
    "        vr.vaccine_reference_id, \\\n",
    "        vr.vaccine_name, \\\n",
    "        vr.description, \\\n",
    "        vr.target_species, \\\n",
    "        vr.first_dose_age_months, \\\n",
    "        vr.booster_interval_months, \\\n",
    "        vr.mandatory, \\\n",
    "        vc.application_date \\\n",
    "    FROM pet p \\\n",
    "    CROSS JOIN vaccine_reference vr \\\n",
    "    LEFT JOIN vaccination_record vc ON p.pet_id = vc.pet_id AND vr.vaccine_reference_id = vc.vaccine_reference_id \\\n",
    "    WHERE p.ignore_recommendation = false AND p.nenabled = TRUE AND vr.nenabled = TRUE AND \\$CONDITIONS\" \\\n",
    "    --target-dir $INPUT_DIR \\\n",
    "    --m 1 \\\n",
    "    --split-by p.pet_id\n",
    "\n",
    "echo \"\\nImportação concluída. Verificando os dados no HDFS:\"\n",
    "hdfs dfs -ls $INPUT_DIR\n",
    "hdfs dfs -cat $INPUT_DIR/part-m-00000 | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Execução do Job MapReduce\n",
    "\n",
    "Com os dados no HDFS, podemos executar nosso job MapReduce. A célula abaixo submete o código Java ou Python para o Hadoop. O resultado será salvo no diretório `/petshop/output_vaccine_recommendation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "INPUT_DIR=/petshop/input_vaccine_recommendation\n",
    "OUTPUT_DIR=/petshop/output_vaccine_recommendation\n",
    "\n",
    "# Remove o diretório de saída se ele já existir\n",
    "hdfs dfs -test -d $OUTPUT_DIR\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Removendo diretório HDFS de saída existente: $OUTPUT_DIR\"\n",
    "    hdfs dfs -rm -r $OUTPUT_DIR\n",
    "fi\n",
    "\n",
    "if [ \"$MAPREDUCE_LANG\" == \"java\" ]; then\n",
    "    JAR_PATH=vaccine-recommendation/target/vaccine-recommendation-1.0-SNAPSHOT.jar\n",
    "\n",
    "    echo \"Executando o job MapReduce (Java)...\"\n",
    "    hadoop jar $JAR_PATH -D log4j.logger.com.petshop.hadoop.RecommendationMapper=DEBUG $INPUT_DIR $OUTPUT_DIR\n",
    "    \n",
    "elif [ \"$MAPREDUCE_LANG\" == \"python\" ]; then\n",
    "    MAPPER_PATH=vaccine-recommendation-python/mapper.py\n",
    "    REDUCER_PATH=vaccine-recommendation-python/reducer.py\n",
    "\n",
    "    echo \"Executando o job MapReduce (Python)...\"\n",
    "    hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "        -file $MAPPER_PATH -mapper $MAPPER_PATH \\\n",
    "        -file $REDUCER_PATH -reducer $REDUCER_PATH \\\n",
    "        -input $INPUT_DIR \\\n",
    "        -output $OUTPUT_DIR\n",
    "fi\n",
    "\n",
    "echo \"\\nJob concluído!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verificação do Resultado\n",
    "\n",
    "Vamos verificar o resultado processado no HDFS. O arquivo `part-r-00000` deve conter o `pet_id`, `vaccine_name`, `description`, `mandatory` e `suggested_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_DIR=/petshop/output_vaccine_recommendation\n",
    "\n",
    "echo \"Conteúdo do diretório de saída:\"\n",
    "hdfs dfs -ls $OUTPUT_DIR\n",
    "\n",
    "echo \"\\nResultado do processamento:\"\n",
    "hdfs dfs -cat $OUTPUT_DIR/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gravando e consultando resultados no Redis\n",
    "\n",
    "A etapa final na arquitetura real seria carregar este resultado no Redis para consulta rápida. O serviço `loader-py` faria isso automaticamente. Abaixo, simulamos como os dados seriam armazenados e consultados usando `redis-cli`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_FILE=/petshop/output_vaccine_recommendation/part-r-00000\n",
    "REDIS_HOST=localhost\n",
    "\n",
    "# Verifica se o arquivo de resultado existe\n",
    "hdfs dfs -test -e $OUTPUT_FILE\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"Erro: Arquivo de resultado não encontrado em $OUTPUT_FILE\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"Limpando recomendações antigas no Redis...\"\n",
    "redis-cli -h $REDIS_HOST KEYS \"recommendation:vaccine:pet:*\" | xargs -r redis-cli -h $REDIS_HOST DEL\n",
    "\n",
    "echo \"Carregando novas recomendações do HDFS para o Redis...\"\n",
    "# Lê o arquivo do HDFS e processa linha por linha\n",
    "hdfs dfs -cat $OUTPUT_FILE | while IFS=$'\\t' read -r pet_id values; do\n",
    "    # Extrai os valores (nome da vacina, descrição, obrigatoriedade e data sugerida)\n",
    "    vaccine_name=$(echo $values | cut -d',' -f1)\n",
    "    description=$(echo $values | cut -d',' -f2)\n",
    "    mandatory=$(echo $values | cut -d',' -f3)\n",
    "    sug_date=$(echo $values | cut -d',' -f4)\n",
    "    \n",
    "    # Monta e executa o comando HSET para o Redis\n",
    "    echo \"Inserindo recomendação para o pet_id: $pet_id para '$vaccine_name' no dia $sug_date\"\n",
    "    redis-cli -h $REDIS_HOST HSET \"recommendation:vaccine:pet:$pet_id:$vaccine_name\" \\\n",
    "        suggested_date \"$sug_date\" \\\n",
    "        description \"$description\" \\\n",
    "        mandatory \"$mandatory\"\n",
    "done\n",
    "\n",
    "echo \"\\nCarga de dados concluída.\"\n",
    "\n",
    "echo \"Todos os registros inseridos:\"\n",
    "# redis-cli -h $REDIS_HOST KEYS \"recommendation:vaccine:pet:*\"\n",
    "\n",
    "for key in $(redis-cli -h $REDIS_HOST KEYS \"recommendation:vaccine:pet:*\");\n",
    "\n",
    "  do echo \"\\n Key : '$key'\" \n",
    "     redis-cli -h $REDIS_HOST HGETALL $key;\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gravando resultados no PostgreSQL\n",
    "\n",
    "Com o resultado processado e validado, a próxima etapa é armazená-lo no PostgreSQL para consumo por outras aplicações. A célula abaixo lê o resultado do HDFS e o insere na tabela `vaccine_recommendation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "# Database connection details\n",
    "DB_HOST = \"localhost\"\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_USER_PWD = \"postgres\"\n",
    "\n",
    "# HDFS output file\n",
    "output_file = \"/petshop/output_vaccine_recommendation/part-r-00000\"\n",
    "\n",
    "try:\n",
    "    # Connect to the database\n",
    "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Truncate the table before inserting new data\n",
    "    cur.execute(\"TRUNCATE TABLE vaccine_recommendation;\")\n",
    "    print(\"Table vaccine_recommendation truncated.\")\n",
    "\n",
    "    # Read the HDFS file\n",
    "    process = Popen([\"hdfs\", \"dfs\", \"-cat\", output_file], stdout=PIPE)\n",
    "    (stdout, stderr) = process.communicate()\n",
    "    exit_code = process.wait()\n",
    "\n",
    "    if exit_code == 0:\n",
    "        # Decode the output and split into lines\n",
    "        lines = stdout.decode(\"utf-8\").strip().split('\\n')\n",
    "        \n",
    "        print(f\"Inserting {len(lines)} records into vaccine_recommendation...\")\n",
    "        \n",
    "        # Process each line and insert into the database\n",
    "        for line in lines:\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            pet_id, values = line.split('\\t')\n",
    "            parts = values.split(',')\n",
    "            vaccine_name = parts[0]\n",
    "            suggested_date = parts[-1]\n",
    "            mandatory = parts[-2].lower() == 'true'\n",
    "            description = ','.join(parts[1:-2])\n",
    "            \n",
    "            # Prepare and execute the INSERT statement\n",
    "            cur.execute(\n",
    "                \"INSERT INTO vaccine_recommendation (pet_id, vaccine_name, description, mandatory, suggested_date) VALUES (%s, %s, %s, %s, %s)\",\n",
    "                (int(pet_id), vaccine_name, description, mandatory, suggested_date)\n",
    "            )\n",
    "        \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "        print(\"Data successfully inserted into vaccine_recommendation.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error reading HDFS file: {stderr}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if 'conn' in locals() and conn is not None:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "from datetime import datetime\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "# Database connection details\n",
    "DB_HOST = \"localhost\"\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_USER_PWD = \"postgres\"\n",
    "\n",
    "# HDFS output file\n",
    "output_file = \"/petshop/output_vaccine_recommendation/part-r-00000\"\n",
    "\n",
    "# Get the execution_id from the environment variable\n",
    "execution_id = os.environ.get('EXECUTION_ID')\n",
    "\n",
    "if execution_id:\n",
    "    try:\n",
    "        # Read the HDFS file to count the number of processed records\n",
    "        process = Popen([\"hdfs\", \"dfs\", \"-cat\", output_file], stdout=PIPE)\n",
    "        (stdout, stderr) = process.communicate()\n",
    "        exit_code = process.wait()\n",
    "\n",
    "        records_processed = 0\n",
    "        if exit_code == 0:\n",
    "            lines = stdout.decode(\"utf-8\").strip().split('\\\\n')\n",
    "            records_processed = len(lines)\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        status = 'COMPLETED'\n",
    "\n",
    "        conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Update the execution_history record\n",
    "        cur.execute(\n",
    "            \"UPDATE execution_history SET end_time = %s, status = %s, records_processed = %s WHERE execution_id = %s\",\n",
    "            (end_time, status, records_processed, execution_id)\n",
    "        )\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Execution finished for pipeline with ID: {execution_id}\")\n",
    "        print(f\"Records processed: {records_processed}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        if 'conn' in locals() and conn is not None:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "else:\n",
    "    print(\"Execution ID not found. The final status could not be updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Consultando dados inseridos no PostgreSQL\n",
    "\n",
    "Execute a célula abaixo para se conectar novamente e fazer uma consulta `SELECT` para verificar se os dados foram inseridos corretamente na tabela `vaccine_recommendation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\"SELECT * FROM vaccine_recommendation;\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    print(\"Registros encontrados:\")\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")\n",
    "finally:\n",
    "    if 'conn' in locals() and conn is not None:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
