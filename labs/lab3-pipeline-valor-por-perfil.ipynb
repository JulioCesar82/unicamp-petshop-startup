{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 3: Cálculo de Valor por Perfil de Pet\n",
                "\n",
                "Este notebook executa o pipeline de cálculo de Valor por Perfil de Pet (LTV). As células abaixo irão compilar o código MapReduce, ingerir dados do PostgreSQL com Sqoop, executa o job no Hadoop e verifica os resultados."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Define qual implementação de MapReduce usar: 'java' ou 'python'\n",
                "MAPREDUCE_LANG = 'java'  # Altere para 'python' para usar a implementação em Python\n",
                "os.environ['MAPREDUCE_LANG'] = MAPREDUCE_LANG\n",
                "\n",
                "print(f\"Usando implementação de MapReduce: {os.environ['MAPREDUCE_LANG'].upper()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Compilando a Aplicação MapReduce\n",
                "\n",
                "A primeira etapa é compilar nosso código-fonte Java em um arquivo `.jar` executável. A célula abaixo usa o Maven para isso. Ela navega até o diretório do projeto e executa o `mvn package`. A flag `-q` é para uma saída mais limpa."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "if [ \"$MAPREDUCE_LANG\" == \"java\" ]; then\n",
                "    cd ltv-by-pet-profile\n",
                "    mvn package -q\n",
                "    echo \"JAR compilado com sucesso em: target/ltv-by-pet-profile-1.0-SNAPSHOT.jar\"\n",
                "fi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Consultando custo em compras por Pet\n",
                "\n",
                "Execute a célula abaixo para se conectar novamente e fazer uma consulta `SELECT` com os dados inseridos anteriormente."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import psycopg2\n",
                "import os\n",
                "\n",
                "# As credenciais e o host são baseados no arquivo docker-compose.txt\n",
                "DB_HOST = \"localhost\" # Nome do serviço no Docker Compose\n",
                "DB_NAME = \"postgres\"\n",
                "DB_USER = \"postgres\"\n",
                "DB_USER_PWD = \"postgres\"\n",
                "\n",
                "try:\n",
                "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
                "    cur = conn.cursor()\n",
                "    \n",
                "    cur.execute(\"\"\"SELECT \n",
                "            CONCAT(p.species, ';', p.animal_type, ';', p.fur_type) AS perfil_pet,\n",
                "            (hc.quantity * hc.price) AS valor_compra \n",
                "            FROM purchase hc \n",
                "            JOIN pet p ON hc.tutor_id = p.tutor_id\n",
                "            WHERE hc.nenabled = TRUE AND p.nenabled = TRUE\"\"\")\n",
                "    rows = cur.fetchall()\n",
                "    \n",
                "    print(\"Custos encontrados:\")\n",
                "    for row in rows:\n",
                "        print(row)\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"Ocorreu um erro: {e}\")\n",
                "finally:\n",
                "    if 'conn' in locals() and conn is not None:\n",
                "        cur.close()\n",
                "        conn.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Montar custo por Pet com base no histórico de Compras"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import psycopg2\n",
                "import os\n",
                "from datetime import datetime\n",
                "\n",
                "# Database connection details\n",
                "DB_HOST = \"localhost\"\n",
                "DB_NAME = \"postgres\"\n",
                "DB_USER = \"postgres\"\n",
                "DB_USER_PWD = \"postgres\"\n",
                "\n",
                "pipeline_name = 'ltv_by_pet_profile'\n",
                "start_time = datetime.now()\n",
                "status = 'RUNNING'\n",
                "\n",
                "try:\n",
                "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
                "    cur = conn.cursor()\n",
                "\n",
                "    # Insert a new record into the execution_history table\n",
                "    cur.execute(\n",
                "        \"INSERT INTO execution_history (target_table, start_time, status) VALUES (%s, %s, %s) RETURNING execution_id\",\n",
                "        (pipeline_name, start_time, status)\n",
                "    )\n",
                "    execution_id = cur.fetchone()[0]\n",
                "    conn.commit()\n",
                "\n",
                "    # Store the execution_id for later use\n",
                "    os.environ['EXECUTION_ID'] = str(execution_id)\n",
                "\n",
                "    print(f\"Execution started for pipeline: {pipeline_name}\")\n",
                "    print(f\"Execution ID: {execution_id}\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")\n",
                "finally:\n",
                "    if 'conn' in locals() and conn is not None:\n",
                "        cur.close()\n",
                "        conn.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Ingestão de histórico de Compras com Sqoop\n",
                "\n",
                "Importar os dados de compras do PostgreSQL para o HDFS. O comando `hdfs dfs -rm` é usado para remover o diretório de destino antes da importação, garantindo que possamos executar esta célula várias vezes sem erros.\n",
                "\n",
                "A query Sqoop já realiza um JOIN para obter o perfil do pet e o valor da compra, simplificando o MapReduce posterior."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "INPUT_DIR=/petshop/input_ltv\n",
                "\n",
                "# Remove o diretório de entrada se ele já existir\n",
                "hdfs dfs -test -d $INPUT_DIR\n",
                "if [ $? -eq 0 ]; then\n",
                "    echo \"Removendo diretório HDFS existente: $INPUT_DIR\"\n",
                "    hdfs dfs -rm -r $INPUT_DIR\n",
                "fi\n",
                "\n",
                "echo \"Iniciando importação com Sqoop...\"\n",
                "sqoop import \\\n",
                "    --connect jdbc:postgresql://localhost:5432/postgres \\\n",
                "    --username postgres \\\n",
                "    --password postgres \\\n",
                "    --query \"SELECT \\\n",
                "                CONCAT(p.species, ';', p.animal_type, ';', p.fur_type) AS perfil_pet, \\\n",
                "                (hc.quantity * hc.price) AS valor_compra \\\n",
                "            FROM purchase hc \\\n",
                "            JOIN pet p ON hc.tutor_id = p.tutor_id \\\n",
                "            WHERE hc.nenabled = TRUE AND p.nenabled = TRUE AND \\$CONDITIONS\" \\\n",
                "    --target-dir $INPUT_DIR \\\n",
                "    --m 1 \\\n",
                "    --split-by hc.purchase_id\n",
                "\n",
                "echo \"\\nImportação concluída. Verificando os dados no HDFS:\"\n",
                "hdfs dfs -ls $INPUT_DIR\n",
                "hdfs dfs -cat $INPUT_DIR/part-m-00000 | head -n 5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Execução do Job MapReduce\n",
                "\n",
                "Com os dados no HDFS, podemos executar nosso job MapReduce. A célula abaixo submete o código Java ou Python para o Hadoop. O resultado será salvo no diretório `/petshop/output_ltv`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "INPUT_DIR=/petshop/input_ltv\n",
                "OUTPUT_DIR=/petshop/output_ltv\n",
                "\n",
                "# Remove o diretório de saída se ele já existir\n",
                "hdfs dfs -test -d $OUTPUT_DIR\n",
                "if [ $? -eq 0 ]; then\n",
                "    echo \"Removendo diretório HDFS de saída existente: $OUTPUT_DIR\"\n",
                "    hdfs dfs -rm -r $OUTPUT_DIR\n",
                "fi\n",
                "\n",
                "if [ \"$MAPREDUCE_LANG\" == \"java\" ]; then\n",
                "    JAR_PATH=ltv-by-pet-profile/target/ltv-by-pet-profile-1.0-SNAPSHOT.jar\n",
                "   \n",
                "    echo \"Executando o job MapReduce (Java)...\"\n",
                "    hadoop jar $JAR_PATH -D log4j.logger.com.petshop.hadoop.LTVMapper=DEBUG $INPUT_DIR $OUTPUT_DIR\n",
                "\n",
                "elif [ \"$MAPREDUCE_LANG\" == \"python\" ]; then\n",
                "    MAPPER_PATH=ltv-by-pet-profile-python/mapper.py\n",
                "    REDUCER_PATH=ltv-by-pet-profile-python/reducer.py\n",
                " \n",
                "    echo \"Executando o job MapReduce (Python)...\"\n",
                "    hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
                "        -file $MAPPER_PATH -mapper $MAPPER_PATH \\\n",
                "        -file $REDUCER_PATH -reducer $REDUCER_PATH \\\n",
                "        -input $INPUT_DIR \\\n",
                "        -output $OUTPUT_DIR\n",
                "fi\n",
                "\n",
                "echo \"\\nJob concluído!\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Verificando resultados\n",
                "\n",
                "Vamos verificar o resultado processado no HDFS. O arquivo `part-r-00000` deve conter o `perfil_pet` e o `valor_total`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "OUTPUT_DIR=/petshop/output_ltv\n",
                "\n",
                "echo \"Conteúdo do diretório de saída:\"\n",
                "hdfs dfs -ls $OUTPUT_DIR\n",
                "\n",
                "echo \"\\nResultado do processamento:\"\n",
                "hdfs dfs -cat $OUTPUT_DIR/part-r-00000"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Gravando resultados no PostgreSQL\n",
                "\n",
                "Com o resultado processado e validado, a próxima etapa é armazená-lo no PostgreSQL para consumo por outras aplicações. A célula abaixo lê o resultado do HDFS e o insere na tabela `ltv_by_pet_profile`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import psycopg2\n",
                "import os\n",
                "from subprocess import Popen, PIPE\n",
                "\n",
                "# Database connection details\n",
                "DB_HOST = \"localhost\"\n",
                "DB_NAME = \"postgres\"\n",
                "DB_USER = \"postgres\"\n",
                "DB_USER_PWD = \"postgres\"\n",
                "\n",
                "# HDFS output file\n",
                "output_file = \"/petshop/output_ltv/part-r-00000\"\n",
                "\n",
                "try:\n",
                "    # Connect to the database\n",
                "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
                "    cur = conn.cursor()\n",
                "\n",
                "    # Truncate the table before inserting new data\n",
                "    cur.execute(\"TRUNCATE TABLE ltv_by_pet_profile;\")\n",
                "    print(\"Table ltv_by_pet_profile truncated.\")\n",
                "\n",
                "    # Read the HDFS file\n",
                "    process = Popen([\"hdfs\", \"dfs\", \"-cat\", output_file], stdout=PIPE)\n",
                "    (stdout, stderr) = process.communicate()\n",
                "    exit_code = process.wait()\n",
                "\n",
                "    if exit_code == 0:\n",
                "        # Decode the output and split into lines\n",
                "        lines = stdout.decode(\"utf-8\").strip().split('\\n')\n",
                "        \n",
                "        print(f\"Inserting {len(lines)} records into ltv_by_pet_profile...\")\n",
                "        \n",
                "        # Process each line and insert into the database\n",
                "        for line in lines:\n",
                "            if not line:\n",
                "                continue\n",
                "                \n",
                "            pet_profile, total_value = line.split('\\t')\n",
                "            \n",
                "            # Prepare and execute the INSERT statement\n",
                "            cur.execute(\n",
                "                \"INSERT INTO ltv_by_pet_profile (pet_profile, total_value) VALUES (%s, %s)\",\n",
                "                (pet_profile, float(total_value))\n",
                "            )\n",
                "        \n",
                "        # Commit the transaction\n",
                "        conn.commit()\n",
                "        print(\"Data successfully inserted into ltv_by_pet_profile.\")\n",
                "\n",
                "    else:\n",
                "        print(f\"Error reading HDFS file: {stderr}\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")\n",
                "finally:\n",
                "    if 'conn' in locals() and conn is not None:\n",
                "        cur.close()\n",
                "        conn.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import psycopg2\n",
                "import os\n",
                "from datetime import datetime\n",
                "from subprocess import Popen, PIPE\n",
                "\n",
                "# Database connection details\n",
                "DB_HOST = \"localhost\"\n",
                "DB_NAME = \"postgres\"\n",
                "DB_USER = \"postgres\"\n",
                "DB_USER_PWD = \"postgres\"\n",
                "\n",
                "# HDFS output file\n",
                "output_file = \"/petshop/output_ltv/part-r-00000\"\n",
                "\n",
                "# Get the execution_id from the environment variable\n",
                "execution_id = os.environ.get('EXECUTION_ID')\n",
                "\n",
                "if execution_id:\n",
                "    try:\n",
                "        # Read the HDFS file to count the number of processed records\n",
                "        process = Popen([\"hdfs\", \"dfs\", \"-cat\", output_file], stdout=PIPE)\n",
                "        (stdout, stderr) = process.communicate()\n",
                "        exit_code = process.wait()\n",
                "\n",
                "        records_processed = 0\n",
                "        if exit_code == 0:\n",
                "            lines = stdout.decode(\"utf-8\").strip().split('\\\\n')\n",
                "            records_processed = len(lines)\n",
                "\n",
                "        end_time = datetime.now()\n",
                "        status = 'COMPLETED'\n",
                "\n",
                "        conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
                "        cur = conn.cursor()\n",
                "\n",
                "        # Update the execution_history record\n",
                "        cur.execute(\n",
                "            \"UPDATE execution_history SET end_time = %s, status = %s, records_processed = %s WHERE execution_id = %s\",\n",
                "            (end_time, status, records_processed, execution_id)\n",
                "        )\n",
                "        conn.commit()\n",
                "\n",
                "        print(f\"Execution finished for pipeline with ID: {execution_id}\")\n",
                "        print(f\"Records processed: {records_processed}\")\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"An error occurred: {e}\")\n",
                "    finally:\n",
                "        if 'conn' in locals() and conn is not None:\n",
                "            cur.close()\n",
                "            conn.close()\n",
                "else:\n",
                "    print(\"Execution ID not found. The final status could not be updated.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Consultando dados inseridos no PostgreSQL\n",
                "\n",
                "Execute a célula abaixo para se conectar novamente e fazer uma consulta `SELECT` para verificar se os dados foram inseridos corretamente na tabela `ltv_by_pet_profile`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
                "    cur = conn.cursor()\n",
                "    \n",
                "    cur.execute(\"SELECT * FROM ltv_by_pet_profile;\")\n",
                "    rows = cur.fetchall()\n",
                "    \n",
                "    print(\"Registros encontrados:\")\n",
                "    for row in rows:\n",
                "        print(row)\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"Ocorreu um erro: {e}\")\n",
                "finally:\n",
                "    if 'conn' in locals() and conn is not None:\n",
                "        cur.close()\n",
                "        conn.close()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
