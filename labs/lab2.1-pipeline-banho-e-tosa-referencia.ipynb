{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.1: Frequência de referência para ir ao Banho e Tosa\n",
    "\n",
    "Este notebook executa o pipeline de geração de dados de referência para a frequência de banho e tosa. As células abaixo irão compilar o código MapReduce, ingerir dados do PostgreSQL com Sqoop, executar o job no Hadoop e armazenar os dados resultantes na tabela `booking_reference`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define qual implementação de MapReduce usar: 'java' ou 'python'\n",
    "MAPREDUCE_LANG = 'java'  # Altere para 'python' para usar a implementação em Python\n",
    "os.environ['MAPREDUCE_LANG'] = MAPREDUCE_LANG\n",
    "\n",
    "print(f\"Usando implementação de MapReduce: {os.environ['MAPREDUCE_LANG'].upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilando a Aplicação MapReduce\n",
    "\n",
    "A primeira etapa é compilar nosso código-fonte Java em um arquivo `.jar` executável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$MAPREDUCE_LANG\" == \"java\" ]; then\n",
    "    cd booking-recommendation-generate-reference\n",
    "    mvn package -q\n",
    "    echo \"JAR compilado com sucesso em: target/booking-recommendation-generate-reference-1.0-SNAPSHOT.jar\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultando histórico de Agendamentos\n",
    "\n",
    "Execute a célula abaixo para se conectar novamente e fazer uma consulta `SELECT` com os dados inseridos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# As credenciais e o host são baseados no arquivo docker-compose.txt\n",
    "DB_HOST = \"localhost\" # Nome do serviço no Docker Compose\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_USER_PWD = \"postgres\"\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\"\"\"SELECT\n",
    "                b.pet_id,\n",
    "                CONCAT(p.species, ';', p.animal_type, ';', p.fur_type) AS pet_profile,\n",
    "                b.booking_date\n",
    "            FROM booking b\n",
    "            JOIN pet p ON b.pet_id = p.pet_id\n",
    "            WHERE b.status = 'Realizado' AND p.nenabled = TRUE AND b.nenabled = TRUE\"\"\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    print(\"Agendamentos encontrados:\")\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")\n",
    "finally:\n",
    "    if 'conn' in locals() and conn is not None:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar recomendação ao Banho e Tosa com base na frequência média de todos os Pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Database connection details\n",
    "DB_HOST = \"localhost\"\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_USER_PWD = \"postgres\"\n",
    "\n",
    "pipeline_name = 'booking_reference'\n",
    "start_time = datetime.now()\n",
    "status = 'RUNNING'\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Insert a new record into the execution_history table\n",
    "    cur.execute(\n",
    "        \"INSERT INTO execution_history (target_table, start_time, status) VALUES (%s, %s, %s) RETURNING execution_id\",\n",
    "        (pipeline_name, start_time, status)\n",
    "    )\n",
    "    execution_id = cur.fetchone()[0]\n",
    "    conn.commit()\n",
    "\n",
    "    # Store the execution_id for later use\n",
    "    os.environ['EXECUTION_ID'] = str(execution_id)\n",
    "\n",
    "    print(f\"Execution started for pipeline: {pipeline_name}\")\n",
    "    print(f\"Execution ID: {execution_id}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if 'conn' in locals() and conn is not None:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ingestão do histórico de Agendamentos com Sqoop\n",
    "\n",
    "Importar os dados de agendamentos do PostgreSQL para o HDFS. A query já combina os dados do pet para formar o perfil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "INPUT_DIR=/petshop/input_booking_reference\n",
    "\n",
    "# Remove o diretório de entrada se ele já existir\n",
    "hdfs dfs -test -d $INPUT_DIR\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Removendo diretório HDFS existente: $INPUT_DIR\"\n",
    "    hdfs dfs -rm -r $INPUT_DIR\n",
    "fi\n",
    "\n",
    "echo \"Iniciando importação com Sqoop...\"\n",
    "sqoop import \\\n",
    "    --connect jdbc:postgresql://localhost:5432/postgres \\\n",
    "    --username postgres \\\n",
    "    --password postgres \\\n",
    "    --query \"SELECT \\\n",
    "                b.pet_id, \\\n",
    "                CONCAT(p.species, ';', p.animal_type, ';', p.fur_type) AS pet_profile, \\\n",
    "                b.booking_date \\\n",
    "            FROM booking b \\\n",
    "            JOIN pet p ON b.pet_id = p.pet_id \\\n",
    "            WHERE b.status = 'Realizado' AND p.nenabled = TRUE AND b.nenabled = TRUE AND \\$CONDITIONS\" \\\n",
    "    --target-dir $INPUT_DIR \\\n",
    "    --m 1 \\\n",
    "    --split-by b.pet_id\n",
    "\n",
    "echo \"\\nImportação concluída. Verificando os dados no HDFS:\"\n",
    "hdfs dfs -ls $INPUT_DIR\n",
    "hdfs dfs -cat $INPUT_DIR/part-m-00000 | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Execução do Job MapReduce\n",
    "\n",
    "Com os dados no HDFS, podemos executar nosso job MapReduce. O resultado será salvo no diretório `/petshop/output_booking_reference`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "INPUT_DIR=/petshop/input_booking_reference\n",
    "OUTPUT_DIR=/petshop/output_booking_reference\n",
    "\n",
    "# Remove o diretório de saída se ele já existir\n",
    "hdfs dfs -test -d $OUTPUT_DIR\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Removendo diretório HDFS de saída existente: $OUTPUT_DIR\"\n",
    "    hdfs dfs -rm -r $OUTPUT_DIR\n",
    "fi\n",
    "\n",
    "if [ \"$MAPREDUCE_LANG\" == \"java\" ]; then\n",
    "    JAR_PATH=booking-recommendation-generate-reference/target/booking-recommendation-generate-reference-1.0-SNAPSHOT.jar\n",
    "   \n",
    "    echo \"Executando o job MapReduce (Java)...\"\n",
    "    hadoop jar $JAR_PATH -D log4j.logger.com.petshop.hadoop.BookingReferenceMapper=DEBUG $INPUT_DIR $OUTPUT_DIR\n",
    "\n",
    "elif [ \"$MAPREDUCE_LANG\" == \"python\" ]; then\n",
    "    MAPPER_PATH=booking-recommendation-generate-reference-python/mapper.py\n",
    "    REDUCER_PATH=booking-recommendation-generate-reference-python/reducer.py\n",
    "  \n",
    "    echo \"Executando o job MapReduce (Python)...\"\n",
    "    hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "        -file $MAPPER_PATH -mapper $MAPPER_PATH \\\n",
    "        -file $REDUCER_PATH -reducer $REDUCER_PATH \\\n",
    "        -input $INPUT_DIR \\\n",
    "        -output $OUTPUT_DIR\n",
    "fi\n",
    "\n",
    "echo \"\\nJob concluído!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verificando resultados\n",
    "\n",
    "Vamos verificar o resultado processado no HDFS. O arquivo `part-r-00000` deve conter o `pet_profile` e a `frequency_days`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_DIR=/petshop/output_booking_reference\n",
    "\n",
    "echo \"Conteúdo do diretório de saída:\"\n",
    "hdfs dfs -ls $OUTPUT_DIR\n",
    "\n",
    "echo \"\\nResultado do processamento:\"\n",
    "hdfs dfs -cat $OUTPUT_DIR/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gravando resultados no PostgreSQL\n",
    "\n",
    "Com o resultado processado e validado, a próxima etapa é armazená-lo no PostgreSQL para consumo pelo pipeline de recomendação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "# Database connection details\n",
    "DB_HOST = \"localhost\"\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_USER_PWD = \"postgres\"\n",
    "\n",
    "# HDFS output file\n",
    "output_file = \"/petshop/output_booking_reference/part-r-00000\"\n",
    "\n",
    "try:\n",
    "    # Connect to the database\n",
    "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Truncate the table before inserting new data\n",
    "    cur.execute(\"TRUNCATE TABLE booking_reference;\")\n",
    "    print(\"Table booking_reference truncated.\")\n",
    "\n",
    "    # Read the HDFS file\n",
    "    process = Popen([\"hdfs\", \"dfs\", \"-cat\", output_file], stdout=PIPE)\n",
    "    (stdout, stderr) = process.communicate()\n",
    "    exit_code = process.wait()\n",
    "\n",
    "    if exit_code == 0:\n",
    "        # Decode the output and split into lines\n",
    "        lines = stdout.decode(\"utf-8\").strip().split('\\n')\n",
    "        \n",
    "        print(f\"Inserting {len(lines)} records into booking_reference...\")\n",
    "        \n",
    "        # Process each line and insert into the database\n",
    "        for line in lines:\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            pet_profile, frequency_days = line.split('\\t')\n",
    "            species, animal_type, fur_type = pet_profile.split(';')\n",
    "            \n",
    "            # Prepare and execute the INSERT statement\n",
    "            cur.execute(\n",
    "                \"INSERT INTO booking_reference (species, animal_type, fur_type, frequency_days) VALUES (%s, %s, %s, %s)\",\n",
    "                (species, animal_type, fur_type, int(frequency_days))\n",
    "            )\n",
    "        \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "        print(\"Data successfully inserted into booking_reference.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error reading HDFS file: {stderr}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if 'conn' in locals() and conn is not None:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "from datetime import datetime\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "# Database connection details\n",
    "DB_HOST = \"localhost\"\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_USER_PWD = \"postgres\"\n",
    "\n",
    "# HDFS output file\n",
    "output_file = \"/petshop/output_booking_reference/part-r-00000\"\n",
    "\n",
    "# Get the execution_id from the environment variable\n",
    "execution_id = os.environ.get('EXECUTION_ID')\n",
    "\n",
    "if execution_id:\n",
    "    try:\n",
    "        # Read the HDFS file to count the number of processed records\n",
    "        process = Popen([\"hdfs\", \"dfs\", \"-cat\", output_file], stdout=PIPE)\n",
    "        (stdout, stderr) = process.communicate()\n",
    "        exit_code = process.wait()\n",
    "\n",
    "        records_processed = 0\n",
    "        if exit_code == 0:\n",
    "            lines = stdout.decode(\"utf-8\").strip().split('\\\\n')\n",
    "            records_processed = len(lines)\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        status = 'COMPLETED'\n",
    "\n",
    "        conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Update the execution_history record\n",
    "        cur.execute(\n",
    "            \"UPDATE execution_history SET end_time = %s, status = %s, records_processed = %s WHERE execution_id = %s\",\n",
    "            (end_time, status, records_processed, execution_id)\n",
    "        )\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Execution finished for pipeline with ID: {execution_id}\")\n",
    "        print(f\"Records processed: {records_processed}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        if 'conn' in locals() and conn is not None:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "else:\n",
    "    print(\"Execution ID not found. The final status could not be updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Consultando dados inseridos no PostgreSQL\n",
    "\n",
    "Execute a célula abaixo para verificar se os dados foram inseridos corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\"SELECT * FROM booking_reference;\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    print(\"Registros encontrados:\")\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")\n",
    "finally:\n",
    "    if 'conn' in locals() and conn is not None:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
